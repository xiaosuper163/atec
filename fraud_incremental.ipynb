{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import xgboost as xgb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "# import missingno as msno\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_curve\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "from sklearn import metrics\n",
    "# from fancyimpute import *\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000):\n",
    "        with pd.option_context(\"display.max_columns\", 1000):\n",
    "            display(df)\n",
    "\n",
    "def batch_save(train_x, train_y, valid_x, valid_y, test, postfix):\n",
    "    train_x.reset_index().to_feather(\"tmp/train_x_{}\".format(postfix))\n",
    "    train_y.reset_index().to_feather(\"tmp/train_y_{}\".format(postfix))\n",
    "    valid_x.reset_index().to_feather(\"tmp/valid_x_{}\".format(postfix))\n",
    "    valid_y.reset_index().to_feather(\"tmp/valid_y_{}\".format(postfix))\n",
    "    test.reset_index().to_feather(\"tmp/test_{}\".format(postfix))\n",
    "    \n",
    "def batch_load(postfix):\n",
    "    train_x = pd.read_feather(\"tmp/train_x_{}\".format(postfix))\n",
    "    train_y = pd.read_feather(\"tmp/train_y_{}\".format(postfix))\n",
    "    valid_x = pd.read_feather(\"tmp/valid_x_{}\".format(postfix))\n",
    "    valid_y = pd.read_feather(\"tmp/valid_y_{}\".format(postfix))\n",
    "    return train_x, train_y, valid_x, valid_y\n",
    "\n",
    "def my_roc(y_true, y_prob):\n",
    "    if isinstance(y_true,pd.core.series.Series):\n",
    "        y_true = np.array(y_true.tolist())\n",
    "    if isinstance(y_true,list):\n",
    "        y_true = np.array(y_true)\n",
    "    sort_index = np.argsort(y_prob)[::-1]\n",
    "    y_prob = y_prob[sort_index]\n",
    "    y_true = y_true[sort_index]\n",
    "    num_p = y_true.sum()\n",
    "    num_n = len(y_true) - num_p\n",
    "    fp = 0\n",
    "    tp = 0\n",
    "    fps = []\n",
    "    tps = []\n",
    "    prob_prev = -99\n",
    "    i = 0\n",
    "    while i < len(y_true):\n",
    "        if y_prob[i]!=prob_prev:\n",
    "            fps.append(fp/num_n)\n",
    "            tps.append(tp/num_p)\n",
    "            prob_prev=y_prob[i]\n",
    "        if y_true[i]==1:\n",
    "            tp+=1\n",
    "        else:\n",
    "            fp+=1\n",
    "        i+=1\n",
    "    fps.append(fp/num_n)\n",
    "    tps.append(tp/num_p)\n",
    "    return np.array(fps), np.array(tps)\n",
    "\n",
    "def my_score3(predictions, xtrain): ##Adapted from SKlearn, conservative (actual should be higher)\n",
    "    ground_truth = xtrain.get_label()\n",
    "    fpr,tpr = my_roc(ground_truth, predictions)\n",
    "#     plt.scatter(fpr, tpr)\n",
    "#     plt.show()\n",
    "    tpr1 = tpr[(fpr>=0.001).argmax()-1]\n",
    "    tpr2 = tpr[(fpr>=0.005).argmax()-1] \n",
    "    tpr3 = tpr[(fpr>=0.01).argmax()-1]\n",
    "    return 'score', 0.4 * tpr1 + 0.3 * tpr2 + 0.3* tpr3\n",
    "\n",
    "def get_ratio(predictions, xtrain):\n",
    "    ratio_predict = (predictions>0.5).sum()/predictions.shape[0]*100\n",
    "    # ratio_true = xtrain.get_label().sum()/xtrain.get_label().shape[0]*100\n",
    "    return 'score', ratio_predict\n",
    "\n",
    "def norm_standardize(df, start=0):\n",
    "    for col in df.columns[start:]:\n",
    "#         avg = df[col].mean()\n",
    "#         std = df[col].std(ddof=0)\n",
    "#         if std != 0:\n",
    "#             df[col] = (df[col]-avg)/std\n",
    "#         else:\n",
    "#             print(col)\n",
    "        a = df[col]\n",
    "        z = a\n",
    "        z[~np.isnan(a)] = zscore(a[~np.isnan(a)])\n",
    "        df[col] = z\n",
    "            \n",
    "def norm_maxmin(df, start=0):\n",
    "    for col in df.columns[start:]:\n",
    "        df[col]=(df[col]-df[col].min())/(df[col].max()-df[col].min())\n",
    "        \n",
    "def process_dates(df,date):\n",
    "    attrs = ['Day', 'Dayofweek', 'Is_month_end', 'Is_month_start']\n",
    "    for attr in attrs:\n",
    "        df[attr] = getattr(df[date].dt, attr.lower())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = load_obj('dict_dtype')\n",
    "\n",
    "my_dict = load_obj('my_dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** Load the training data and test data ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"atec_anti_fraud_train.csv\",parse_dates=['date'], dtype = dtype)\n",
    "test = pd.read_csv(\"atec_anti_fraud_test_a.csv\",parse_dates=['date'], dtype = dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_ratio = load_obj('df_missing_ratio')\n",
    "# display_all(df_missing_ratio)\n",
    "\n",
    "selected_cols = ['f'+str(item) for item in df_missing_ratio[df_missing_ratio['positive_missing_ratio']<0.1].index.tolist()]\n",
    "all_nan_cols = ['f'+str(item) for item in df_missing_ratio[df_missing_ratio['positive_missing_ratio']>0.9].index.tolist()]\n",
    "\n",
    "# # use the columns with no or few missing values\n",
    "# data = data.drop(all_nan_cols, axis=1)\n",
    "# test = test.drop(all_nan_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** Perform one hot encoding for the columns with no more than 10 unique values ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding for the columns with no more than 10 unique values\n",
    "for col in data.columns[3:]:\n",
    "    data_unique = data[col].unique()\n",
    "    test_unique = test[col].unique()\n",
    "    if data_unique[~np.isnan(data_unique)].min() == test_unique[~np.isnan(test_unique)].min() and \\\n",
    "    data_unique[~np.isnan(data_unique)].max() == test_unique[~np.isnan(test_unique)].max() and \\\n",
    "    data_unique.shape[0] == test_unique.shape[0] and \\\n",
    "    data_unique.shape[0] <= 10:\n",
    "        data[col].fillna(-1.0)\n",
    "        test[col].fillna(-1.0)\n",
    "        for num in data[col].unique():\n",
    "            new_col = '{}={}'.format(col, num)\n",
    "            data[new_col] = data[col].apply(lambda x: np.isnan(x) if np.isnan(num) else x==num)\n",
    "            test[new_col] = test[col].apply(lambda x: np.isnan(x) if np.isnan(num) else x==num)\n",
    "            data.drop([col], axis=1)\n",
    "            test.drop([col], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** Perform normalization ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization with maxmin\n",
    "norm_maxmin(data, 3)\n",
    "norm_maxmin(test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization with zscore\n",
    "norm_standardize(data, 3)\n",
    "norm_standardize(test, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** Sort the training data and remove the unlabeled data ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporially ignore the rows without labels\n",
    "data.sort_values('date',inplace=True)\n",
    "# unlabeled = data[data['label']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the prediction on the unlabeled training data, set the labels for unlabeled data\n",
    "data.loc[data['label']==-1,'label'] = pd.Series((pred_xgunlabeled>0.5).astype(int), name='label', index=data.loc[data['label']==-1,'label'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['label']!=-1]\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** calculate the weight of each row with trained RF model ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = load_obj('train_test_shift')\n",
    "weights = rf_model.predict_proba(data.fillna(-1).iloc[:,3:].values)[:,1:]\n",
    "\n",
    "print(weights.shape, data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from Katrina\n",
    "weights = load_obj('weights')\n",
    "\n",
    "weights.weight.values.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.iloc[weights.weight.values.argsort()[weights.shape[0]//10-weights.shape[0]:],:]\n",
    "valid = data.iloc[weights.weight.values.argsort()[:weights.shape[0]//10],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = 1./weights - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** data segementation based on missing pattern ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cate = pd.read_csv(\"obj/id_cate_train.csv\")\n",
    "test_cate = pd.read_csv(\"obj/id_cate_test.csv\")\n",
    "data = data.merge(train_cate, how='inner', on='id')\n",
    "data = data[data['label']!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = load_obj('weights')\n",
    "weights = weights.drop('label', axis=1)\n",
    "data = data.merge(weights, how='inner', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = data['weight']\n",
    "data = data.drop('weight', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_dates(data,'date')\n",
    "test = process_dates(test,'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_cate = [[0, 4, 9, 12, 13, 32, 33], [1, 5], [18, 31], [3, 8, 15, 17, 19, 20, 29, 32],\\\n",
    " [16, 22, 30], [7, 26], [11, 21, 23, 24, 25, 27],[2, 6, 10, 14, 28]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_mapping = list()\n",
    "for i in range(34):\n",
    "    temp = data[data['cate']==i].head(1)\\\n",
    "        .drop(['id','label','date', 'cate', 'Day', 'Dayofweek', 'Is_month_end', 'Is_month_start'], axis=1).values[0]\n",
    "    temp_list = list()\n",
    "    for j in range(297):\n",
    "        if np.isnan(temp[j]):\n",
    "            temp_list.append(0)\n",
    "        else:\n",
    "            temp_list.append(1)\n",
    "    cate_mapping.append(tuple(temp_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "0 [0, 0.6907915622357733, 0.8121878617703036, 0.9679060415469871, 0.9982440723942094, 0.6882472016116852, 0.7723284457212329, 0.48122650318578775, 0.9468620919467723, 0.9894176915848294, 0.8100249075659298, 0.8335087534664906, 0.9929576588559561, 0.9911892555667041, 0.7464329414686255, 0.9375697353156422, 0.2513123449750173, 0.9588004201156959, 0.6352234031660237, 0.9357002678873535, 0.9450108801221079, 0.7694837640638656, 0.4017505554096096, 0.6958523739384593, 0.7947194142390264, 0.7836039099693787, 0.48485845284809215, 0.8229173606481639, 0.8035011108192193, 0.9660917830792961, 0.2581988897471611, 0.5833959866102805, 0.9724114853089061, 0.9840838646332837]\n",
      "4 0.9982440723942094\n",
      "12 0.9929576588559561\n",
      "13 0.9911892555667041\n",
      "9 0.9894176915848294\n",
      "=========================\n",
      "1 [0.6907915622357733, 0, 0.8505317485662418, 0.713696921585171, 0.6869183914799764, 0.996316746232607, 0.8944271909999155, 0.6966305460192359, 0.6981799174515236, 0.6930462415879095, 0.846532256117865, 0.4082937701376479, 0.6956908545644069, 0.6969320524371696, 0.8644378215075663, 0.704521372964301, 0.3638034375544995, 0.7204748222288193, 0.5357429050935312, 0.7006207424084403, 0.6942916087048013, 0.41462428362148235, 0.2275747818408781, 0.13139036155660133, 0.4282221187731151, 0.43429649477995014, 0.7018882096342189, 0.4135487324359245, 0.859726953621095, 0.7097795722139673, 0.37377250079820096, 0.5833369430134643, 0.6812956938992034, 0.7019641181630336]\n",
      "5 0.996316746232607\n",
      "6 0.8944271909999155\n",
      "14 0.8644378215075663\n",
      "28 0.859726953621095\n",
      "=========================\n",
      "2 [0.8121878617703036, 0.8505317485662418, 0, 0.7587773633277245, 0.8092887660547694, 0.8473990242990481, 0.950923403407938, 0.5925063964103965, 0.7422802325048486, 0.8165082557553335, 0.99733687942632, 0.5960549796723262, 0.8005449798156936, 0.8194074514114278, 0.9190397648170291, 0.7490222440620204, 0.30942637387763805, 0.7659833879352873, 0.7821138840728852, 0.7459787502643745, 0.7392398700618465, 0.5105546021465772, 0.49465225266224133, 0.2855876112297998, 0.5272985257181991, 0.534778311027372, 0.5969782062382213, 0.6037265305873012, 0.9893045053244827, 0.7557305203714435, 0.3179053786698709, 0.7183017797614808, 0.7839801888190022, 0.8077637460938354]\n",
      "10 0.99733687942632\n",
      "28 0.9893045053244827\n",
      "6 0.950923403407938\n",
      "14 0.9190397648170291\n",
      "=========================\n",
      "3 [0.9679060415469871, 0.713696921585171, 0.7587773633277245, 0, 0.9659771111625637, 0.7110681947099657, 0.7979374159983899, 0.4971830761761255, 0.9782582722940951, 0.9562749403324301, 0.7563281601566222, 0.7828603169094882, 0.9747707094199438, 0.9582231192584709, 0.7711832651396772, 0.9686577984544258, 0.25964539344474935, 0.990592453150992, 0.5535631645709838, 0.9667263429742008, 0.97634567773925, 0.7949984100047702, 0.25265243302483587, 0.6251526437927266, 0.8210708272559601, 0.8095867536036436, 0.5009354545128695, 0.7709101495726802, 0.766980600253966, 0.9981255841064991, 0.26676028319286693, 0.6027403090467842, 0.95460085828309, 0.9651417875838685]\n",
      "29 0.9981255841064991\n",
      "17 0.990592453150992\n",
      "8 0.9782582722940951\n",
      "20 0.97634567773925\n",
      "=========================\n",
      "4 [0.9982440723942094, 0.6869183914799764, 0.8092887660547694, 0.9659771111625637, 0, 0.6894578396653825, 0.769135884148571, 0.48207298845422053, 0.9449750998415306, 0.9911580934428084, 0.8114497545907275, 0.830757863964531, 0.9911644146645897, 0.9893865851573571, 0.7433474237219933, 0.9356341306744776, 0.25175440748900674, 0.9568209849086691, 0.6308073734803931, 0.9373461799208589, 0.9466731696744003, 0.766554869119551, 0.4024572411895647, 0.697076389614328, 0.7916944645477939, 0.7804966689882864, 0.4784619039875321, 0.8200935650568854, 0.800539947148808, 0.9677911542837427, 0.24503974655279853, 0.5783972186390622, 0.9706553467343192, 0.9822430911439491]\n",
      "0 0.9982440723942094\n",
      "12 0.9911644146645897\n",
      "9 0.9911580934428084\n",
      "13 0.9893865851573571\n",
      "=========================\n",
      "5 [0.6882472016116852, 0.996316746232607, 0.8473990242990481, 0.7110681947099657, 0.6894578396653825, 0, 0.8911327886790067, 0.699205898780101, 0.6956083436402523, 0.6956083436402523, 0.8496617760555312, 0.4036867138796656, 0.6931284486033922, 0.6943650748294137, 0.8612538776448218, 0.7019264419631215, 0.36514837167011077, 0.7178211306255334, 0.5296977535773846, 0.7032108464077431, 0.6968583147178248, 0.4099457958749615, 0.2284160962880643, 0.1318760946791574, 0.4233901974057256, 0.42939603209735455, 0.693968327664163, 0.40888238086483086, 0.856560361080241, 0.7124035352189665, 0.3554093266554553, 0.576754755602384, 0.6787863089679409, 0.6993786061802352]\n",
      "1 0.996316746232607\n",
      "6 0.8911327886790067\n",
      "14 0.8612538776448218\n",
      "28 0.856560361080241\n",
      "=========================\n",
      "6 [0.7723284457212329, 0.8944271909999155, 0.950923403407938, 0.7979374159983899, 0.769135884148571, 0.8911327886790067, 0, 0.6230853024407226, 0.7805888779733992, 0.7759971786912028, 0.9478539857794418, 0.528706534968914, 0.777806021065467, 0.7791937224739794, 0.9664708656063741, 0.7876788407748296, 0.3253956867279843, 0.8055153392903581, 0.6937425836565869, 0.7844782740554297, 0.7773916042160114, 0.5369040243586827, 0.3166318911223304, 0.18280750757349773, 0.5545120919607786, 0.5623779045828748, 0.6277878997390947, 0.5355112748677799, 0.9612039551927888, 0.7947333272722508, 0.3343122879619487, 0.7553729114113887, 0.7617117421682539, 0.7848197429891193]\n",
      "14 0.9664708656063741\n",
      "28 0.9612039551927888\n",
      "2 0.950923403407938\n",
      "10 0.9478539857794418\n",
      "=========================\n",
      "7 [0.48122650318578775, 0.6966305460192359, 0.5925063964103965, 0.4971830761761255, 0.48207298845422053, 0.699205898780101, 0.6230853024407226, 0, 0.48637345711392, 0.48637345711392, 0.5940885257860045, 0.5773502691896257, 0.4846394998757918, 0.4855041562276122, 0.6021937915964947, 0.4907911087303427, 0.5222329678670936, 0.5019047688023743, 0.7575704874652, 0.4916891718944416, 0.4872474442646632, 0.5863019699779288, 0.3266793038882825, 0.18860838403857944, 0.6055300708194984, 0.6141195788629906, 0.9925092578236595, 0.584781080334426, 0.5989120571285178, 0.49811675413689877, 0.5083042452524145, 0.8248711239545368, 0.4746113912415564, 0.4890096469218257]\n",
      "26 0.9925092578236595\n",
      "31 0.8248711239545368\n",
      "18 0.7575704874652\n",
      "5 0.699205898780101\n",
      "=========================\n",
      "8 [0.9468620919467723, 0.6981799174515236, 0.7422802325048486, 0.9782582722940951, 0.9449750998415306, 0.6956083436402523, 0.7805888779733992, 0.48637345711392, 0, 0.9354838709677418, 0.7398842792421889, 0.7658395810674835, 0.9535775100800434, 0.9373896931180504, 0.8076693315360826, 0.9909989167736739, 0.25400025400038095, 0.9690552617670587, 0.5415277449788625, 0.9891888715790902, 0.998206276582811, 0.8295613557843402, 0.2471593326217755, 0.6115607452367585, 0.8032193289024989, 0.7919849388524857, 0.4900442522626168, 0.7541492310149525, 0.7503051168875328, 0.9764246094405583, 0.2609604537529375, 0.5896356933701159, 0.9758168014715309, 0.9441579376406296]\n",
      "20 0.998206276582811\n",
      "15 0.9909989167736739\n",
      "19 0.9891888715790902\n",
      "3 0.9782582722940951\n",
      "=========================\n",
      "9 [0.9894176915848294, 0.6930462415879095, 0.8165082557553335, 0.9562749403324301, 0.9911580934428084, 0.6956083436402523, 0.7759971786912028, 0.48637345711392, 0.9354838709677418, 0, 0.818688522001712, 0.816895553138649, 0.9821491208689586, 0.9982126884730385, 0.7499786649977909, 0.9439807199924412, 0.25400025400038095, 0.9653565775618409, 0.6364346693566012, 0.9457080420591302, 0.9371648855687542, 0.7517899786795583, 0.4060474750214883, 0.7032948570222723, 0.7764453512724157, 0.7874593106304715, 0.4827301589452643, 0.8274094420278336, 0.8076813905318735, 0.9580707633984425, 0.24722569302909872, 0.5835569748817642, 0.9618265964325126, 0.9910054688976074]\n",
      "13 0.9982126884730385\n",
      "4 0.9911580934428084\n",
      "33 0.9910054688976074\n",
      "0 0.9894176915848294\n",
      "=========================\n",
      "10 [0.8100249075659298, 0.846532256117865, 0.99733687942632, 0.7563281601566222, 0.8114497545907275, 0.8496617760555312, 0.9478539857794418, 0.5940885257860045, 0.7398842792421889, 0.818688522001712, 0, 0.5924496577650307, 0.7983202148705191, 0.8172252705693475, 0.916073262104709, 0.7466045287425527, 0.31025261399701154, 0.7635109249528453, 0.7773831588605933, 0.7479706863878035, 0.7412138118135831, 0.5066403971048989, 0.4959730887990136, 0.28635019632892067, 0.5232559521341829, 0.5306783931098709, 0.5896383618094194, 0.6000748062313725, 0.9865551657632555, 0.757748495980765, 0.30197771971277465, 0.7127948654452296, 0.781801458471836, 0.805518919742284]\n",
      "2 0.99733687942632\n",
      "28 0.9865551657632555\n",
      "6 0.9478539857794418\n",
      "14 0.916073262104709\n",
      "=========================\n",
      "11 [0.8335087534664906, 0.4082937701376479, 0.5960549796723262, 0.7828603169094882, 0.830757863964531, 0.4036867138796656, 0.528706534968914, 0.5773502691896257, 0.7658395810674835, 0.816895553138649, 0.5924496577650307, 0, 0.8224622525509626, 0.8196825662288478, 0.510979462503153, 0.7513290345414274, 0.30151134457776374, 0.7683424142535055, 0.7621076569672299, 0.7484026724341384, 0.7629534441872838, 0.9231861823449956, 0.4400862294233521, 0.7259540086406278, 0.9534625892455924, 0.9401267913629437, 0.5817076915288627, 0.987293003493631, 0.5815425174522868, 0.7799730868772032, 0.30977345909486176, 0.6999278462091576, 0.8054439516938245, 0.8084900286844691]\n",
      "27 0.987293003493631\n",
      "24 0.9534625892455924\n",
      "25 0.9401267913629437\n",
      "21 0.9231861823449956\n",
      "=========================\n",
      "12 [0.9929576588559561, 0.6956908545644069, 0.8005449798156936, 0.9747707094199438, 0.9911644146645897, 0.6931284486033922, 0.777806021065467, 0.4846394998757918, 0.9535775100800434, 0.9821491208689586, 0.7983202148705191, 0.8224622525509626, 0, 0.9839587824764855, 0.7517268584529917, 0.9442192493845813, 0.2530947243657587, 0.9656005083040349, 0.6174771609151997, 0.9423365231559093, 0.9517131689289846, 0.774941164108077, 0.3694172870644411, 0.6804748761289083, 0.8003557927683123, 0.7891614541472128, 0.4882972083691116, 0.8115774840698359, 0.8091997716649661, 0.9729435837096878, 0.260030110492976, 0.5875335986455299, 0.9793081070841199, 0.9910632702779121]\n",
      "0 0.9929576588559561\n",
      "4 0.9911644146645897\n",
      "33 0.9910632702779121\n",
      "13 0.9839587824764855\n",
      "=========================\n",
      "13 [0.9911892555667041, 0.6969320524371696, 0.8194074514114278, 0.9582231192584709, 0.9893865851573571, 0.6943650748294137, 0.7791937224739794, 0.4855041562276122, 0.9373896931180504, 0.9982126884730385, 0.8172252705693475, 0.8196825662288478, 0.9839587824764855, 0, 0.75306803143448, 0.9459038524177654, 0.253546276418555, 0.9673232581274396, 0.6408699444616558, 0.9440177671744181, 0.9354898799661135, 0.7547592055307001, 0.4053217416888888, 0.7020378500174637, 0.7795119555779044, 0.7905694150420948, 0.4891683905218266, 0.8302323254883022, 0.8106434833777775, 0.9563583924793758, 0.2604940361258638, 0.5885818306986477, 0.9635988127158426, 0.992831448793946]\n",
      "9 0.9982126884730385\n",
      "33 0.992831448793946\n",
      "0 0.9911892555667041\n",
      "4 0.9893865851573571\n",
      "=========================\n",
      "14 [0.7464329414686255, 0.8644378215075663, 0.9190397648170291, 0.7711832651396772, 0.7433474237219933, 0.8612538776448218, 0.9664708656063741, 0.6021937915964947, 0.8076693315360826, 0.7499786649977909, 0.916073262104709, 0.510979462503153, 0.7517268584529917, 0.75306803143448, 0, 0.8150052617267791, 0.3144854510165755, 0.7785071072231646, 0.6704819953345842, 0.8120103359775734, 0.8046749522613109, 0.5830961710461189, 0.3060154978915819, 0.17667813008390215, 0.535919781506535, 0.54352186024011, 0.6067387148780504, 0.5175560453634362, 0.9289756185994451, 0.7680866067350459, 0.32310308632943197, 0.7300459115473719, 0.78813730374619, 0.7585054163516662]\n",
      "6 0.9664708656063741\n",
      "28 0.9289756185994451\n",
      "2 0.9190397648170291\n",
      "10 0.916073262104709\n",
      "=========================\n",
      "15 [0.9375697353156422, 0.704521372964301, 0.7490222440620204, 0.9686577984544258, 0.9356341306744776, 0.7019264419631215, 0.7876788407748296, 0.4907911087303427, 0.9909989167736739, 0.9439807199924412, 0.7466045287425527, 0.7513290345414274, 0.9442192493845813, 0.9459038524177654, 0.8150052617267791, 0, 0.2563072973150283, 0.977857034316389, 0.5464463540907558, 0.9981735144570327, 0.9891564054135398, 0.8152967493727135, 0.24940424095157937, 0.6171154527875513, 0.7880005393141865, 0.7991784102357002, 0.49449524511895515, 0.760999046770085, 0.7571200171744374, 0.9667727655218501, 0.2633307154386489, 0.5949912592132309, 0.9670333932278385, 0.9527335718130336]\n",
      "19 0.9981735144570327\n",
      "8 0.9909989167736739\n",
      "20 0.9891564054135398\n",
      "17 0.977857034316389\n",
      "=========================\n",
      "16 [0.2513123449750173, 0.3638034375544995, 0.30942637387763805, 0.25964539344474935, 0.25175440748900674, 0.36514837167011077, 0.3253956867279843, 0.5222329678670936, 0.25400025400038095, 0.25400025400038095, 0.31025261399701154, 0.30151134457776374, 0.2530947243657587, 0.253546276418555, 0.3144854510165755, 0.2563072973150283, 0, 0.2621112169983114, 0.3956282840374723, 0.2567762955065478, 0.2544566789039914, 0.30618621784789735, 0.6255432421712244, 0.36115755925730764, 0.316227766016838, 0.3207134902949093, 0.5183210553488161, 0.30539195913557254, 0.3127716210856122, 0.260132990857236, 0.9733285267845752, 0.43077489517064294, 0.2478577154316083, 0.2553769592276246]\n",
      "30 0.9733285267845752\n",
      "22 0.6255432421712244\n",
      "7 0.5222329678670936\n",
      "26 0.5183210553488161\n",
      "=========================\n",
      "17 [0.9588004201156959, 0.7204748222288193, 0.7659833879352873, 0.990592453150992, 0.9568209849086691, 0.7178211306255334, 0.8055153392903581, 0.5019047688023743, 0.9690552617670587, 0.9653565775618409, 0.7635109249528453, 0.7683424142535055, 0.9656005083040349, 0.9673232581274396, 0.7785071072231646, 0.977857034316389, 0.2621112169983114, 0, 0.5588202926543053, 0.9759072360173193, 0.967091271822669, 0.7802554101632737, 0.25505184520755175, 0.6310896492338176, 0.8058443224934924, 0.8172753093650327, 0.5056927830607182, 0.7782313979078672, 0.7742645300943536, 0.9886647348175106, 0.26929367606660504, 0.6084644670262904, 0.9456204059866888, 0.9743076322798874]\n",
      "3 0.990592453150992\n",
      "29 0.9886647348175106\n",
      "15 0.977857034316389\n",
      "19 0.9759072360173193\n",
      "=========================\n",
      "18 [0.6352234031660237, 0.5357429050935312, 0.7821138840728852, 0.5535631645709838, 0.6308073734803931, 0.5296977535773846, 0.6937425836565869, 0.7575704874652, 0.5415277449788625, 0.6364346693566012, 0.7773831588605933, 0.7621076569672299, 0.6174771609151997, 0.6408699444616558, 0.6704819953345842, 0.5464463540907558, 0.3956282840374723, 0.5588202926543053, 0, 0.5418024809213567, 0.5369080492621874, 0.6527880562455257, 0.5774593988133563, 0.3333963393509707, 0.6741966054512084, 0.6837601555447341, 0.763288081691424, 0.7719163959133092, 0.7630713484319351, 0.5488851669033799, 0.40646942234853023, 0.9184107255849996, 0.604700389623541, 0.6230451469985845]\n",
      "31 0.9184107255849996\n",
      "2 0.7821138840728852\n",
      "10 0.7773831588605933\n",
      "27 0.7719163959133092\n",
      "=========================\n",
      "19 [0.9357002678873535, 0.7006207424084403, 0.7459787502643745, 0.9667263429742008, 0.9373461799208589, 0.7032108464077431, 0.7844782740554297, 0.4916891718944416, 0.9891888715790902, 0.9457080420591302, 0.7479706863878035, 0.7484026724341384, 0.9423365231559093, 0.9440177671744181, 0.8120103359775734, 0.9981735144570327, 0.2567762955065478, 0.9759072360173193, 0.5418024809213567, 0, 0.9909663912006343, 0.8124207484594868, 0.2498606077393724, 0.6182446677351863, 0.784931344843112, 0.7960656789167189, 0.48800605507688205, 0.758035020795087, 0.7540436197848916, 0.9685417931047159, 0.24992769341858886, 0.5899348361554086, 0.965267120715541, 0.950833868448844]\n",
      "15 0.9981735144570327\n",
      "20 0.9909663912006343\n",
      "8 0.9891888715790902\n",
      "17 0.9759072360173193\n",
      "=========================\n",
      "20 [0.9450108801221079, 0.6942916087048013, 0.7392398700618465, 0.97634567773925, 0.9466731696744003, 0.6968583147178248, 0.7773916042160114, 0.4872474442646632, 0.998206276582811, 0.9371648855687542, 0.7412138118135831, 0.7629534441872838, 0.9517131689289846, 0.9354898799661135, 0.8046749522613109, 0.9891564054135398, 0.2544566789039914, 0.967091271822669, 0.5369080492621874, 0.9909663912006343, 0, 0.8267236372706799, 0.2476034647546832, 0.6126596872645729, 0.8001923230055152, 0.7888743329947838, 0.4835975992835958, 0.7511872289610051, 0.7472318847060975, 0.9781791923641091, 0.24766994440811754, 0.584605595628463, 0.9740664560238452, 0.9422444072480894]\n",
      "8 0.998206276582811\n",
      "19 0.9909663912006343\n",
      "15 0.9891564054135398\n",
      "29 0.9781791923641091\n",
      "=========================\n",
      "21 [0.7694837640638656, 0.41462428362148235, 0.5105546021465772, 0.7949984100047702, 0.766554869119551, 0.4099457958749615, 0.5369040243586827, 0.5863019699779288, 0.8295613557843402, 0.7517899786795583, 0.5066403971048989, 0.9231861823449956, 0.774941164108077, 0.7547592055307001, 0.5830961710461189, 0.8152967493727135, 0.30618621784789735, 0.7802554101632737, 0.6527880562455257, 0.8124207484594868, 0.8267236372706799, 0, 0.2553769592276246, 0.6266283136583128, 0.9682458365518544, 0.9547032697824669, 0.5907269532815762, 0.9090939756974499, 0.5160742717724913, 0.7920664140466075, 0.3145764348029479, 0.710780087884666, 0.8094999088258619, 0.7602088012497519]\n",
      "24 0.9682458365518544\n",
      "25 0.9547032697824669\n",
      "11 0.9231861823449956\n",
      "27 0.9090939756974499\n",
      "=========================\n",
      "22 [0.4017505554096096, 0.2275747818408781, 0.49465225266224133, 0.25265243302483587, 0.4024572411895647, 0.2284160962880643, 0.3166318911223304, 0.3266793038882825, 0.2471593326217755, 0.4060474750214883, 0.4959730887990136, 0.4400862294233521, 0.3694172870644411, 0.4053217416888888, 0.3060154978915819, 0.24940424095157937, 0.6255432421712244, 0.25505184520755175, 0.5774593988133563, 0.2498606077393724, 0.2476034647546832, 0.2553769592276246, 0, 0.5773502691896257, 0.2637521893583148, 0.2674935420361694, 0.32423223344850904, 0.4457503779182722, 0.4565217391304348, 0.25312689810571776, 0.6088590823425644, 0.3592910994280177, 0.36177334411922885, 0.37274843911917926]\n",
      "16 0.6255432421712244\n",
      "30 0.6088590823425644\n",
      "18 0.5774593988133563\n",
      "23 0.5773502691896257\n",
      "=========================\n",
      "23 [0.6958523739384593, 0.13139036155660133, 0.2855876112297998, 0.6251526437927266, 0.697076389614328, 0.1318760946791574, 0.18280750757349773, 0.18860838403857944, 0.6115607452367585, 0.7032948570222723, 0.28635019632892067, 0.7259540086406278, 0.6804748761289083, 0.7020378500174637, 0.17667813008390215, 0.6171154527875513, 0.36115755925730764, 0.6310896492338176, 0.3333963393509707, 0.6182446677351863, 0.6126596872645729, 0.6266283136583128, 0.5773502691896257, 0, 0.6471789394828489, 0.6563592411295426, 0.18719556726145026, 0.7352974305214053, 0.26357294897787265, 0.6263266404020191, 0.35152495508902815, 0.20743681297220257, 0.6663945628601023, 0.6866109324565026]\n",
      "27 0.7352974305214053\n",
      "11 0.7259540086406278\n",
      "9 0.7032948570222723\n",
      "13 0.7020378500174637\n",
      "=========================\n",
      "24 [0.7947194142390264, 0.4282221187731151, 0.5272985257181991, 0.8210708272559601, 0.7916944645477939, 0.4233901974057256, 0.5545120919607786, 0.6055300708194984, 0.8032193289024989, 0.7764453512724157, 0.5232559521341829, 0.9534625892455924, 0.8003557927683123, 0.7795119555779044, 0.535919781506535, 0.7880005393141865, 0.316227766016838, 0.8058443224934924, 0.6741966054512084, 0.784931344843112, 0.8001923230055152, 0.9682458365518544, 0.2637521893583148, 0.6471789394828489, 0, 0.9860132971832694, 0.6101001739241043, 0.938908220803657, 0.5329992159949278, 0.818042674851397, 0.3248931448269655, 0.7340905181848414, 0.7837949164097464, 0.7851402738348248]\n",
      "25 0.9860132971832694\n",
      "21 0.9682458365518544\n",
      "11 0.9534625892455924\n",
      "27 0.938908220803657\n",
      "=========================\n",
      "25 [0.7836039099693787, 0.43429649477995014, 0.534778311027372, 0.8095867536036436, 0.7804966689882864, 0.42939603209735455, 0.5623779045828748, 0.6141195788629906, 0.7919849388524857, 0.7874593106304715, 0.5306783931098709, 0.9401267913629437, 0.7891614541472128, 0.7905694150420948, 0.54352186024011, 0.7991784102357002, 0.3207134902949093, 0.8172753093650327, 0.6837601555447341, 0.7960656789167189, 0.7888743329947838, 0.9547032697824669, 0.2674935420361694, 0.6563592411295426, 0.9860132971832694, 0, 0.6187545093630775, 0.9522267331341505, 0.5405598661980923, 0.8064722079072195, 0.32950178841916555, 0.7445036697597363, 0.7728322098446587, 0.7962775715882575]\n",
      "24 0.9860132971832694\n",
      "21 0.9547032697824669\n",
      "27 0.9522267331341505\n",
      "11 0.9401267913629437\n",
      "=========================\n",
      "26 [0.48485845284809215, 0.7018882096342189, 0.5969782062382213, 0.5009354545128695, 0.4784619039875321, 0.693968327664163, 0.6277878997390947, 0.9925092578236595, 0.4900442522626168, 0.4827301589452643, 0.5896383618094194, 0.5817076915288627, 0.4882972083691116, 0.4891683905218266, 0.6067387148780504, 0.49449524511895515, 0.5183210553488161, 0.5056927830607182, 0.763288081691424, 0.48800605507688205, 0.4835975992835958, 0.5907269532815762, 0.32423223344850904, 0.18719556726145026, 0.6101001739241043, 0.6187545093630775, 0, 0.5891945850628275, 0.6034322122513918, 0.49438548995794374, 0.53252426193765, 0.8310966547186532, 0.4781934148224149, 0.49270033812491526]\n",
      "7 0.9925092578236595\n",
      "31 0.8310966547186532\n",
      "18 0.763288081691424\n",
      "1 0.7018882096342189\n",
      "=========================\n",
      "27 [0.8229173606481639, 0.4135487324359245, 0.6037265305873012, 0.7709101495726802, 0.8200935650568854, 0.40888238086483086, 0.5355112748677799, 0.584781080334426, 0.7541492310149525, 0.8274094420278336, 0.6000748062313725, 0.987293003493631, 0.8115774840698359, 0.8302323254883022, 0.5175560453634362, 0.760999046770085, 0.30539195913557254, 0.7782313979078672, 0.7719163959133092, 0.758035020795087, 0.7511872289610051, 0.9090939756974499, 0.4457503779182722, 0.7352974305214053, 0.938908220803657, 0.9522267331341505, 0.5891945850628275, 0, 0.5890272851062883, 0.7679443958989769, 0.3137604115482421, 0.7089362972617002, 0.7947844096765238, 0.8188957339143996]\n",
      "11 0.987293003493631\n",
      "25 0.9522267331341505\n",
      "24 0.938908220803657\n",
      "21 0.9090939756974499\n",
      "=========================\n",
      "28 [0.8035011108192193, 0.859726953621095, 0.9893045053244827, 0.766980600253966, 0.800539947148808, 0.856560361080241, 0.9612039551927888, 0.5989120571285178, 0.7503051168875328, 0.8076813905318735, 0.9865551657632555, 0.5815425174522868, 0.8091997716649661, 0.8106434833777775, 0.9289756185994451, 0.7571200171744374, 0.3127716210856122, 0.7742645300943536, 0.7630713484319351, 0.7540436197848916, 0.7472318847060975, 0.5160742717724913, 0.4565217391304348, 0.26357294897787265, 0.5329992159949278, 0.5405598661980923, 0.6034322122513918, 0.5890272851062883, 0, 0.7639008174976125, 0.3213422934585757, 0.726067430094119, 0.7924558966421203, 0.816496580927726]\n",
      "2 0.9893045053244827\n",
      "10 0.9865551657632555\n",
      "6 0.9612039551927888\n",
      "14 0.9289756185994451\n",
      "=========================\n",
      "29 [0.9660917830792961, 0.7097795722139673, 0.7557305203714435, 0.9981255841064991, 0.9677911542837427, 0.7124035352189665, 0.7947333272722508, 0.49811675413689877, 0.9764246094405583, 0.9580707633984425, 0.757748495980765, 0.7799730868772032, 0.9729435837096878, 0.9563583924793758, 0.7680866067350459, 0.9667727655218501, 0.260132990857236, 0.9886647348175106, 0.5488851669033799, 0.9685417931047159, 0.9781791923641091, 0.7920664140466075, 0.25312689810571776, 0.6263266404020191, 0.818042674851397, 0.8064722079072195, 0.49438548995794374, 0.7679443958989769, 0.7639008174976125, 0, 0.2531948607591389, 0.5976467299570755, 0.9528115392623747, 0.9632635968986716]\n",
      "3 0.9981255841064991\n",
      "17 0.9886647348175106\n",
      "20 0.9781791923641091\n",
      "8 0.9764246094405583\n",
      "=========================\n",
      "30 [0.2581988897471611, 0.37377250079820096, 0.3179053786698709, 0.26676028319286693, 0.24503974655279853, 0.3554093266554553, 0.3343122879619487, 0.5083042452524145, 0.2609604537529375, 0.24722569302909872, 0.30197771971277465, 0.30977345909486176, 0.260030110492976, 0.2604940361258638, 0.32310308632943197, 0.2633307154386489, 0.9733285267845752, 0.26929367606660504, 0.40646942234853023, 0.24992769341858886, 0.24766994440811754, 0.3145764348029479, 0.6088590823425644, 0.35152495508902815, 0.3248931448269655, 0.32950178841916555, 0.53252426193765, 0.3137604115482421, 0.3213422934585757, 0.2531948607591389, 0, 0.4425791326529006, 0.25464959529175096, 0.262374883916401]\n",
      "16 0.9733285267845752\n",
      "22 0.6088590823425644\n",
      "26 0.53252426193765\n",
      "7 0.5083042452524145\n",
      "=========================\n",
      "31 [0.5833959866102805, 0.5833369430134643, 0.7183017797614808, 0.6027403090467842, 0.5783972186390622, 0.576754755602384, 0.7553729114113887, 0.8248711239545368, 0.5896356933701159, 0.5835569748817642, 0.7127948654452296, 0.6999278462091576, 0.5875335986455299, 0.5885818306986477, 0.7300459115473719, 0.5949912592132309, 0.43077489517064294, 0.6084644670262904, 0.9184107255849996, 0.5899348361554086, 0.584605595628463, 0.710780087884666, 0.3592910994280177, 0.20743681297220257, 0.7340905181848414, 0.7445036697597363, 0.8310966547186532, 0.7089362972617002, 0.726067430094119, 0.5976467299570755, 0.4425791326529006, 0, 0.5753764163378751, 0.5928315741948289]\n",
      "18 0.9184107255849996\n",
      "26 0.8310966547186532\n",
      "7 0.8248711239545368\n",
      "6 0.7553729114113887\n",
      "=========================\n",
      "32 [0.9724114853089061, 0.6812956938992034, 0.7839801888190022, 0.95460085828309, 0.9706553467343192, 0.6787863089679409, 0.7617117421682539, 0.4746113912415564, 0.9758168014715309, 0.9618265964325126, 0.781801458471836, 0.8054439516938245, 0.9793081070841199, 0.9635988127158426, 0.78813730374619, 0.9670333932278385, 0.2478577154316083, 0.9456204059866888, 0.604700389623541, 0.965267120715541, 0.9740664560238452, 0.8094999088258619, 0.36177334411922885, 0.6663945628601023, 0.7837949164097464, 0.7728322098446587, 0.4781934148224149, 0.7947844096765238, 0.7924558966421203, 0.9528115392623747, 0.25464959529175096, 0.5753764163378751, 0, 0.9705562952164601]\n",
      "12 0.9793081070841199\n",
      "8 0.9758168014715309\n",
      "20 0.9740664560238452\n",
      "0 0.9724114853089061\n",
      "=========================\n",
      "33 [0.9840838646332837, 0.7019641181630336, 0.8077637460938354, 0.9651417875838685, 0.9822430911439491, 0.6993786061802352, 0.7848197429891193, 0.4890096469218257, 0.9441579376406296, 0.9910054688976074, 0.805518919742284, 0.8084900286844691, 0.9910632702779121, 0.992831448793946, 0.7585054163516662, 0.9527335718130336, 0.2553769592276246, 0.9743076322798874, 0.6230451469985845, 0.950833868448844, 0.9422444072480894, 0.7602088012497519, 0.37274843911917926, 0.6866109324565026, 0.7851402738348248, 0.7962775715882575, 0.49270033812491526, 0.8188957339143996, 0.816496580927726, 0.9632635968986716, 0.262374883916401, 0.5928315741948289, 0.9705562952164601, 0]\n",
      "13 0.992831448793946\n",
      "12 0.9910632702779121\n",
      "9 0.9910054688976074\n",
      "0 0.9840838646332837\n"
     ]
    }
   ],
   "source": [
    "for i in range(34):\n",
    "    print('=========================')\n",
    "    temp_list = list()\n",
    "    for j in range(34):\n",
    "        if j==i:\n",
    "            temp_list.append(0)\n",
    "        else:\n",
    "            temp_list.append(cosine_similarity([list(cate_mapping[i])], [list(cate_mapping[j])])[0][0])\n",
    "    print(i, temp_list)\n",
    "    print(np.argmax(temp_list), temp_list[np.argmax(temp_list)])\n",
    "    print(np.array(temp_list).argsort()[32], temp_list[np.array(temp_list).argsort()[32]])\n",
    "    print(np.array(temp_list).argsort()[31], temp_list[np.array(temp_list).argsort()[31]])\n",
    "    print(np.array(temp_list).argsort()[30], temp_list[np.array(temp_list).argsort()[30]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++    0    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    1    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "++++++++++++++++    2    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    3    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    4    ++++++++++++++++\n",
      "(1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    5    ++++++++++++++++\n",
      "(1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "++++++++++++++++    6    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    7    ++++++++++++++++\n",
      "(1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "++++++++++++++++    8    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    9    ++++++++++++++++\n",
      "(1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    10    ++++++++++++++++\n",
      "(1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    11    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    12    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    13    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    14    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    15    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    16    ++++++++++++++++\n",
      "(1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "++++++++++++++++    17    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    18    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    19    ++++++++++++++++\n",
      "(1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    20    ++++++++++++++++\n",
      "(1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    21    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    22    ++++++++++++++++\n",
      "(1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "++++++++++++++++    23    ++++++++++++++++\n",
      "(1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "++++++++++++++++    24    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    25    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    26    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "++++++++++++++++    27    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    28    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    29    ++++++++++++++++\n",
      "(1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    30    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "++++++++++++++++    31    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    32    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "++++++++++++++++    33    ++++++++++++++++\n",
      "(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "for i in range(34):\n",
    "    print('++++++++++++++++    {}    ++++++++++++++++'.format(i))\n",
    "    print(cate_mapping[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** incrementally train model ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fraction = 10\n",
    "fraction_size = data.shape[0]//10\n",
    "\n",
    "x_batches = []\n",
    "y_batches = []\n",
    "unlabeled = []\n",
    "for i in range(num_fraction):\n",
    "    if i!=num_fraction-1:\n",
    "        data_portion = data.iloc[i*fraction_size:(i+1)*fraction_size,:]\n",
    "    else:\n",
    "        data_portion = data.iloc[i*fraction_size:,:]\n",
    "    unlabeled.append(data_portion[data_portion['label']==-1].iloc[:,3:])\n",
    "    x_batches.append(data_portion[data_portion['label']!=-1].iloc[:,3:])\n",
    "    y_batches.append(data_portion[data_portion['label']!=-1]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial model\n",
    "# set up the parameters\n",
    "params = {'max_depth': 8, 'eta': 0.1, 'silent': 1, 'objective': 'binary:logistic'}\n",
    "params['nthread'] = 4\n",
    "params['eval_metric'] = ['error', 'auc']\n",
    "params[\"scale_pos_weight\"] = 5\n",
    "num_rounds = 50\n",
    "early_stopping_rounds = 20\n",
    "\n",
    "# set up the random seed for testing\n",
    "params[\"seed\"] = 10\n",
    "\n",
    "xgtrain = xgb.DMatrix(x_batches[0].values, label=y_batches[0].values)\n",
    "xgb_model = xgb.train(params, xgtrain,\\\n",
    "                      num_rounds,\\\n",
    "                      [(xgtrain, 'current'),\\\n",
    "                       (xgb.DMatrix(x_batches[1].values, label=y_batches[1].values), 'next')],\\\n",
    "                      # feval=get_ratio,\\\n",
    "                      early_stopping_rounds=early_stopping_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the unlabeled data of the first fraction\n",
    "pred_unlabeled = xgb_model.predict(xgb.DMatrix(unlabeled[0].values))\n",
    "unlabeled[0]['label'] = pd.Series((pred_unlabeled>0.5).astype(int), name='label',\\\n",
    "                                      index = unlabeled[0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incrementally train the model\n",
    "for fraction_id in range(1,num_fraction):\n",
    "    print('=========================================================================')\n",
    "    print('=====       '+str(num_fraction)+' fractions in total, training on the fraction {}'.format(fraction_id+1)+'       =====')\n",
    "    print('=========================================================================')\n",
    "#     xgtrain = xgb.DMatrix(x_batches[fraction_id].append(unlabeled[fraction_id-1].iloc[:,:-1]).values,\\\n",
    "#                           label=y_batches[fraction_id].append(unlabeled[fraction_id-1]['label']).values)\n",
    "    xgtrain = xgb.DMatrix(x_batches[fraction_id].values,\\\n",
    "                          label=y_batches[fraction_id].values)\n",
    "    if fraction_id != num_fraction-1:\n",
    "        xgb_model = xgb.train(params, xgtrain,\\\n",
    "            num_rounds,\\\n",
    "            [(xgtrain, 'current'),\\\n",
    "            (xgb.DMatrix(x_batches[fraction_id+1].values, label=y_batches[fraction_id+1].values), 'next')],\\\n",
    "            # feval=my_score3,\\\n",
    "            early_stopping_rounds=early_stopping_rounds,\\\n",
    "            xgb_model = xgb_model)\n",
    "#         pred_unlabeled = xgb_model.predict(xgb.DMatrix(unlabeled[fraction_id].values))\n",
    "#         unlabeled[fraction_id]['label'] = pd.Series((pred_unlabeled>0.5).astype(int), name='label',\\\n",
    "#                                                     index = unlabeled[fraction_id].index)\n",
    "    else:\n",
    "        num_rounds = 50\n",
    "        xgb_model = xgb.train(params, xgtrain,\\\n",
    "            num_rounds,\\\n",
    "            [(xgtrain, 'current')],\\\n",
    "            # feval=my_score3,\\\n",
    "            early_stopping_rounds=early_stopping_rounds,\\\n",
    "            xgb_model = xgb_model,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** Create validation set and training set ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.iloc[:len(data) * 8 // 10]\n",
    "valid = data.iloc[len(data) * 8 // 10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weights = weights.weight.values[:len(data) * 8 // 10]\n",
    "train_weights /= np.mean(train_weights) # Normalizing the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train[['label']]\n",
    "train_x = train.iloc[:,3:]\n",
    "valid_y = valid[['label']]\n",
    "valid_x = valid.iloc[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgtrain = xgb.DMatrix(train_x.values, weight=train_weights, label=train_y.values)\n",
    "xgval_1 = xgb.DMatrix(valid_x.iloc[:valid_x.shape[0] // 2,:].values,\\\n",
    "                      label=valid_y.iloc[:valid_x.shape[0] // 2,:].values)\n",
    "xgval_2 = xgb.DMatrix(valid_x.iloc[valid_x.shape[0] // 2:,:].values,\\\n",
    "                      label=valid_y.iloc[valid_x.shape[0] // 2:,:].values)\n",
    "xgval = xgb.DMatrix(valid_x.values,\\\n",
    "                      label=valid_y.iloc[:,:].values)\n",
    "evallist = [(xgtrain, 'train'), (xgval_1, 'val_1'), (xgval_2, 'val_2'), (xgval, 'val')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal xgtrain and xgval\n",
    "xgtrain = xgb.DMatrix(train_x.values, label=train_y.values)\n",
    "xgval_1 = xgb.DMatrix(valid_x.iloc[:valid_x.shape[0] // 2,:].values,\\\n",
    "                      label=valid_y.iloc[:valid_x.shape[0] // 2,:].values)\n",
    "xgval_2 = xgb.DMatrix(valid_x.iloc[valid_x.shape[0] // 2:,:].values,\\\n",
    "                      label=valid_y.iloc[valid_x.shape[0] // 2:,:].values)\n",
    "xgval = xgb.DMatrix(valid_x.values, label=valid_y.iloc[:,:].values)\n",
    "evallist = [(xgtrain, 'train'), (xgval_1, 'val_1'), (xgval_2, 'val_2'), (xgval, 'val')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** Train the model ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the parameters\n",
    "params = {'max_depth': 6, 'eta': 0.1, 'silent': 1, 'objective': 'binary:logistic'}\n",
    "params['nthread'] = 4\n",
    "params['eval_metric'] = ['logloss', 'auc']\n",
    "params[\"colsample_bytree \"] = 0.5\n",
    "params[\"scale_pos_weight\"] = 2\n",
    "num_rounds = 300\n",
    "early_stopping_rounds = 1000\n",
    "\n",
    "# set up the random seed for testing\n",
    "#params[\"seed\"] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time xgb_model = xgb.train(params, xgtrain, num_rounds, evallist,\\\n",
    "    feval=my_score3, early_stopping_rounds=early_stopping_rounds)#, xgb_model = xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** save or load the model ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "xgb_model.save_model('model_log/0020.model')\n",
    "# dump model with feature map\n",
    "xgb_model.dump_model('model_log/dumpraw0020.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "xgb_model = xgb.Booster({'nthread': 4})  # init model\n",
    "xgb_model.load_model('model_log/0020.model')  # load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** train the existing model on the validation set ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgval = xgb.DMatrix(valid_x.values, label=valid_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time xgb_model = xgb.train(params, xgval, num_rounds, [(xgval, 'validation')], feval=my_score3, early_stopping_rounds=early_stopping_rounds, xgb_model = xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** predict on the testset ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "test_whole = pd.read_feather(\"tmp/test_native\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgtest = xgb.DMatrix(test.iloc[:,2:].values)\n",
    "\n",
    "# make predictions\n",
    "preds = xgb_model.predict(xgtest)#, ntree_limit=xgb_model.best_ntree_limit)\n",
    "\n",
    "res = pd.concat([test.id, pd.Series(list(preds), name='score')], axis=1)\n",
    "\n",
    "res.to_csv(\"submission/0020.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the positive ratio of the test data\n",
    "print('Ratio of positive label in unlabeled data: {}%'.format((preds>0.5).sum()/preds.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** predict on the unlabeled training set ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgunlabeled = xgb.DMatrix(unlabeled.iloc[:,3:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgunlabeled = xgb_model.predict(xgunlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the labels for the unlabeled training data\n",
    "unlabeled['label'] = pd.Series((pred_xgunlabeled>0.5).astype(int), name='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.concat([unlabeled.id, pd.Series(list(pred_xgunlabeled), name='score')], axis=1)\n",
    "res.to_csv(\"Yabin_unlabeled0011.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the positive ratio of the unlabeled data\n",
    "print('Ratio of positive label in unlabeled data: {}%'.format((pred_xgunlabeled>0.5).sum()/res.shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
